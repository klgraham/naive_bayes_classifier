## Learning about and Implementing a Naive Bayes Classifier

I'm a physicist working with a bunch of computer scientists who specialize in machine learning. I don't even know enough machine learning to be dangerous.

First, I'm trying out basic spam detection. I wrote some simple Scala code and tested on the lemm-stop subset of the Lingspam Dataset. Punctuation, numbers, and the "Subject" header were deleted. Tab and newline characters were replaced by a single space. See http://csmining.org/index.php/ling-spam-datasets.html for more info.

There is some sensitivity to the smoothing parameter alpha. With alpha equal to one, Naive Bayes correctly labelled all the spam emails in the test set. But only labelled 16% of the non-spam emails in the test set. With alpha set to zero, all non-spam emails were correctly labelled and 16% of spam emails were labelled correctly. This would make one think that alpha equals 1/2 would be optimal then, but that would be wrong. The best quality is with small, nonzero alpha. Perhaps the limit of alpha going to zero.

Here's how the performance changes with alpha.

| Alpha | Total Correct | Spam Correct | Non-Spam Correct |
| ------- | ------- | ------- | ------- |
| 1.0 | 0.5807692307692308 | 1.0 | 0.16153846153846155 |
| 0.8 | 0.5884615384615385 | 1.0 | 0.17692307692307693 |
| 0.6 | 0.6038461538461538 | 1.0 | 0.2076923076923077 |
| 0.5 | 0.6115384615384616 | 1.0 | 0.2230769230769231 |
| 0.25 | 0.6538461538461539 | 1.0 | 0.3076923076923077 |
| 0.125 | 0.7230769230769231 | 1.0 | 0.4461538461538462 |
| 0.0625 | 0.7653846153846153 | 1.0 | 0.5307692307692308 |
| 0.03125 | 0.8 | 0.9923076923076923 | 0.6076923076923076 |
| 0.015625 | 0.85 | 0.9923076923076923 | 0.7076923076923077 |
| 0.0078125 | 0.8846153846153846 | 0.9923076923076923 | 0.7769230769230769 |
| 0.00390625 | 0.9076923076923077 | 0.9923076923076923 | 0.823076923076923 |
| 0.001953125 | 0.9230769230769231 | 0.9923076923076923 | 0.8538461538461538 |
| 9.765625E-4 | 0.9461538461538461 | 0.9923076923076923 | 0.9 |
| 4.8828125E-4 | 0.9615384615384616 | 0.9923076923076923 | 0.9307692307692308 |
| 2.44140625E-4 | 0.9653846153846154 | 0.9923076923076923 | 0.9384615384615385 |
| 1.220703125E-4 | 0.9653846153846154 | 0.9923076923076923 | 0.9384615384615385 |
| 6.103515625E-5 | 0.9653846153846154 | 0.9923076923076923 | 0.9384615384615385 |
| 3.0517578125E-5 | 0.9653846153846154 | 0.9923076923076923 | 0.9384615384615385 |
| 1.52587890625E-5 | 0.9692307692307692 | 0.9846153846153847 | 0.9538461538461539 |
| 7.62939453125E-6 | 0.9692307692307692 | 0.9846153846153847 | 0.9538461538461539 |
| 3.814697265625E-6 | 0.9692307692307692 | 0.9846153846153847 | 0.9538461538461539 |
| 1.9073486328125E-6 | 0.9730769230769231 | 0.9846153846153847 | 0.9615384615384616 |
| 9.5367431640625E-7 | 0.9769230769230769 | 0.9846153846153847 | 0.9692307692307692 |
| 4.76837158203125E-7 | 0.9807692307692307 | 0.9846153846153847 | 0.9769230769230769 |
| 2.384185791015625E-7 | 0.9807692307692307 | 0.9846153846153847 | 0.9769230769230769 |
| 1.1920928955078125E-7 | 0.9846153846153847 | 0.9846153846153847 | 0.9846153846153847 |
| 1.0E-7 | 0.9846153846153847 | 0.9846153846153847 | 0.9846153846153847 |
| 5.9604644775390625E-8 | 0.9807692307692307 | 0.9769230769230769 | 0.9846153846153847 |
| 2.9802322387695312E-8 | 0.9807692307692307 | 0.9769230769230769 | 0.9846153846153847 |
| 1.4901161193847656E-8 | 0.9807692307692307 | 0.9769230769230769 | 0.9846153846153847 |
| 0.0 | 0.5807692307692308 | 0.16153846153846155 | 1.0 |

This is interesting dependence on alpha. The best quality appears around 10^{-7}, if it decreases any more the quality will also decrease.